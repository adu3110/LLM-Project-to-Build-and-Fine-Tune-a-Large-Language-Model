{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6270f25c",
   "metadata": {},
   "source": [
    "# **LLM Project to Build and Fine Tune a Large Language Model**\n",
    "\n",
    "In today's data-driven world, the ability to process and generate natural language text at scale has become a transformative force across industries. Large Language Models (LLMs) represent a cutting-edge advancement in natural language processing, enabling businesses to extract valuable insights, automate tasks, and enhance user experiences. By harnessing the power of LLMs, organizations can improve customer service, automate content creation, and gain a competitive edge in the digital landscape.\n",
    "\n",
    "This project builds the foundation for Large Language Models by diving deep into the details of their inner workings. Moreover, It shows how to optimize their use through prompt engineering and fine-tuning techniques such as LoRA. \n",
    "\n",
    "Prompt engineering techniques involve crafting specific instructions or queries given to the language model to influence its output will be introduced to guide LLMs in generating desired responses through zero-shot, one-shot, and few-shot inferences.\n",
    "\n",
    "Fine-tuning entails training a pre-trained language model on a specific task or dataset to adapt it for a particular application. It explores full fine-tuning and Parameter Efficient Fine Tuning (PEFT), a technique that optimizes the fine-tuning process by focusing on a subset of the model's parameters, making it more resource-efficient.\n",
    "\n",
    "The project also involves the application of Retrieval Augmented Generation (RAG) using OpenAI's GPT-3.5 Turbo, resulting in the development of a chatbot for online shopping for knowledge grounding. Knowledge grounding with Retrieval Augmented Generation (RAG) is implemented to mitigate hallucinations and provide trustworthy and reliable responses. This is achieved by incorporating information from external sources to validate and support the generated text.\n",
    "\n",
    "For example, in the context of an e-commerce chatbot using RAG, knowledge grounding ensures that product information, availability, and prices are sourced from a trusted database or e-commerce platform. This prevents the chatbot from generating inaccurate or fictional details and instead provides responses based on real-world data.\n",
    "\n",
    "\n",
    "![image](https://images.pexels.com/photos/18069697/pexels-photo-18069697/free-photo-of-an-artist-s-illustration-of-artificial-intelligence-ai-this-illustration-depicts-language-models-which-generate-text-it-was-created-by-wes-cockx-as-part-of-the-visualising-ai-project-l.png?auto=compress&cs=tinysrgb&w=1260&h=750&dpr=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8406e",
   "metadata": {},
   "source": [
    "## **Learning Outcomes**\n",
    "\n",
    "* Understand Large Language Models (LLMs) and how they work.\n",
    "* Gain practical experience in implementing generative AI projects.\n",
    "* Understand fundamental NLP concepts like RNNs, Transformers, and Attention Mechanism.\n",
    "* Explore tokenization, embeddings, and internal workings of Transformers.\n",
    "* Generate text and summarize dialogues using LLMs.\n",
    "* Learn optimization techniques like Prompt Engineering, Fine-Tuning, and PEFT.\n",
    "* Apply Prompt Engineering techniques for better responses.\n",
    "* Fine-tune LLMs for improved performance on tasks.\n",
    "* Evaluate model performance using the ROUGE metric.\n",
    "* Understand RLHF for improved model output.\n",
    "* Implement Retrieval Augmented Generation (RAG) for knowledge grounding.\n",
    "* Build a chatbot application for online shopping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b9628-f0e8-4243-a55e-0365f4dcdb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python- 3.8.10 \n",
    "# !pip install --upgrade pip\n",
    "# !pip install transformers\n",
    "# !pip install datasets --quiet\n",
    "# !pip install torchdata\n",
    "# !pip install torch\n",
    "# !pip install streamlit\n",
    "# !pip install openai\n",
    "# !pip install langchain\n",
    "# !pip install unstructured\n",
    "# !pip install sentence-transformers\n",
    "# !pip install chromadb\n",
    "# !pip install evaluate==0.4.0\n",
    "# !pip install rouge_score==0.1.2\n",
    "# !pip install loralib==0.1.1\n",
    "# !pip install peft==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820c1620-b5bd-43d9-988a-b4775c66ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4466e3-0b30-4e1b-a79a-9f6170370afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForCausalLM, \n",
    "                          AutoTokenizer, GenerationConfig, TrainingArguments, Trainer)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63104742",
   "metadata": {},
   "source": [
    "## **Refresher**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1445726f",
   "metadata": {},
   "source": [
    "## **Recurrent Neural Networks**\n",
    "\n",
    "RNN were created because there were a few issues in the feed-forward neural network:\n",
    "\n",
    "Cannot handle sequential data\n",
    "Considers only the current input\n",
    "Cannot memorize previous inputs\n",
    "The solution to these issues is the RNN. An RNN can handle sequential data, accepting the current input data, and previously received inputs. RNNs can memorize previous inputs due to their internal memory.\n",
    "\n",
    "RNN works on the principle of saving the output of a particular layer and feeding this back to the input in order to predict the output of the layer.\n",
    "\n",
    "Below is how you can convert a Feed-Forward Neural Network into a Recurrent Neural Network\n",
    "\n",
    "<img src=\"images/rnn.png\" \n",
    "     align=\"center\" \n",
    "     width=\"700\" />\n",
    "\n",
    "\n",
    "The nodes in different layers of the neural network are compressed to form a single layer of recurrent neural networks. A, B, and C are the parameters of the network.\n",
    "\n",
    "<img src=\"images/rnn_animation.gif\" \n",
    "     align=\"center\" \n",
    "     width=\"450\" />\n",
    "\n",
    "\n",
    "The four commonly used types of Recurrent Neural Networks are:\n",
    "\n",
    "**One-to-One**: The simplest type of RNN is One-to-One, which allows a single input and a single output. It has fixed input and output sizes and acts as a traditional neural network. The One-to-One application can be found in Image Classification.\n",
    "\n",
    "**One-to-Many**: One-to-Many is a type of RNN that gives multiple outputs when given a single input. It takes a fixed input size and gives a sequence of data outputs. Its applications can be found in Music Generation and Image Captioning.\n",
    "\n",
    "**Many-to-One**: Many-to-One is used when a single output is required from multiple input units or a sequence of them. It takes a sequence of inputs to display a fixed output. Sentiment Analysis is a common example of this type of Recurrent Neural Network.\n",
    "\n",
    "**Many-to-Many**: Many-to-Many is used to generate a sequence of output data from a sequence of input units.\n",
    "\n",
    "This type of RNN is further divided into ﻿the following two subcategories:\n",
    "\n",
    "    1. Equal Unit Size: In this case, the number of both the input and output units is the same. A common application can be found in Name-Entity Recognition.\n",
    "    2. Unequal Unit Size: In this case, inputs and outputs have different numbers of units. Its application can be found in Machine Translation.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/types_rnn.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e03fa",
   "metadata": {},
   "source": [
    "## **LSTM**\n",
    "\n",
    "Now, even though RNNs are quite powerful, they suffer from Vanishing gradient problem which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: Long Short Term Networks(LSTM).\n",
    "\n",
    "**What is Vanishing Gradient problem?**\n",
    "\n",
    "Vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n",
    "\n",
    "<img src=\"images/decay.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\" />\n",
    "\n",
    "\n",
    "**Fixing the Vanishing/Exploding Gradient with LSTMs**\n",
    "\n",
    "Long short-term memory (LSTM) units (or blocks) are a building unit for layers of a recurrent neural network (RNN). A RNN composed of LSTM units is often called an LSTM network. A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell is responsible for \"remembering\" values over arbitrary time intervals; hence the word \"memory\" in LSTM. Each of the three gates can be thought of as a \"conventional\" artificial neuron, as in a multi-layer (or feedforward) neural network: that is, they compute an activation (using an activation function) of a weighted sum. Intuitively, they can be thought as regulators of the flow of values that goes through the connections of the LSTM; hence the denotation \"gate\". There are connections between these gates and the cell.\n",
    "\n",
    "The expression long short-term refers to the fact that LSTM is a model for the short-term memory which can last for a long period of time. An LSTM is well-suited to classify, process and predict time series given time lags of unknown size and duration between important events. LSTMs were developed to deal with the exploding and vanishing gradient problem when training traditional RNNs.\n",
    "\n",
    "\n",
    "<img src=\"images/lstm.png\" \n",
    "     align=\"center\" \n",
    "     width=\"750\" />\n",
    "\n",
    "More on LSTMs: https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baea43b",
   "metadata": {},
   "source": [
    "## **Attention Mechanism**\n",
    "\n",
    "In a typical neural network, all parts of the input sequence are treated equally. However, in many tasks, certain parts of the input may be more relevant than others. For example, in a sentence, the word \"dog\" might be more important than \"the\" for understanding the meaning.\n",
    "\n",
    "The attention mechanism addresses this by allowing the model to focus on specific parts of the input when processing it. It does this by assigning weights or scores to different elements of the input sequence. These weights reflect how much attention the model should pay to each element. The weighted sum of the input elements then becomes the basis for making predictions.\n",
    "\n",
    "To know more refer to the project [Build a Text Classification Model with Attention Mechanism NLP](https://www.projectpro.io/project-use-case/multi-class-text-classification-model-with-attention-mechanism-in-nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339596d8",
   "metadata": {},
   "source": [
    "## **Transformers**\n",
    "\n",
    "Transformers are a type of neural network architecture that heavily rely on attention mechanisms. They were introduced in the paper [\"Attention is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf).\n",
    "\n",
    "Quoting from the paper: \n",
    "\n",
    "*The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.*\n",
    "\n",
    "Transformers are a class of powerful neural network architectures that have revolutionized natural language processing (NLP) and various other sequential data tasks. What sets Transformers apart is their unique attention mechanism, which allows them to process input data in parallel rather than sequentially. This means they can consider the relationships between all elements in a sequence simultaneously, making them exceptionally efficient. The attention mechanism works by assigning weights to different parts of the input, highlighting the most relevant information. Transformers are composed of multiple layers, each containing a multi-head self-attention sub-layer and a feedforward neural network sub-layer. These layers are stacked to form the core of the model. \n",
    "\n",
    "Refer to the project [NLP Project for Multi Class Text Classification using BERT Model](https://www.projectpro.io/project-use-case/nlp-project-for-multi-class-text-classification-using-bert) to implement transformers from scratch.\n",
    "\n",
    "Read more https://towardsdatascience.com/transformers-89034557de14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c6467",
   "metadata": {},
   "source": [
    "## **Tokenizers**\n",
    "\n",
    "Tokenization is a fundamental step in Natural Language Processing (NLP) that involves breaking down a text into smaller units, known as tokens. Tokens can be words, subwords, or characters, depending on the level of granularity required for the task at hand. In this tutorial, we'll explore various aspects of tokenization, including different tokenization techniques, libraries, and considerations for tokenizing text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1715f3",
   "metadata": {},
   "source": [
    "## **Embeddings**\n",
    "\n",
    "Word embeddings are a crucial concept in Natural Language Processing (NLP). They represent words as numerical vectors in a continuous vector space. This tutorial will provide a conceptual understanding of word embeddings, their significance, and different methods for generating them.\n",
    "\n",
    "Word embeddings represent words in a continuous vector space, where similar words are positioned closer to each other. This enables algorithms to understand the context and relationships between words.\n",
    "\n",
    "Unlike one-hot encoding (a sparse representation), where each word is represented as a binary vector, word embeddings are dense vectors with continuous values. This allows for more nuanced representation of word meanings.\n",
    "\n",
    "Word embeddings capture semantic relationships between words. Words with similar meanings will have similar vector representations.\n",
    "\n",
    "\n",
    "To know more refer to the projects [Word2Vec and FastText Word Embedding with Gensim in Python](https://www.projectpro.io/project-use-case/word-embedding-with-word2vec-and-fasttext) and [Skip Gram Model Python Implementation for Word Embeddings](https://www.projectpro.io/project-use-case/skip-gram-model-word-embeddings-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a4dce-09af-47d9-9ef3-af01da5437b4",
   "metadata": {},
   "source": [
    "## **Text Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c965e87-bb27-472c-a7ed-e6638635243f",
   "metadata": {},
   "source": [
    "In recent years, there has been an increasing interest in open-ended language generation thanks to the rise of large transformer-based language models trained on millions of webpages, including OpenAI's ChatGPT and Meta's LLaMA. The results on conditioned open-ended language generation are impressive, having shown to generalize to new tasks, handle code, or take non-text data as input. Besides the improved transformer architecture and massive unsupervised training data, better decoding methods have also played an important role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24385ec6-7b69-48d2-8427-8fbdfd3e230f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Currently most prominent decoding methods, mainly Greedy search, Beam search, and Sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768d0e5b-7fe2-4fcc-a5e6-de43233a04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b36582ee-16e1-45e1-b95b-a129748f4873",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ea16bf-6bfc-41be-8192-fb1612c77e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 133kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.05MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 531kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:01<00:00, 1.34MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 548M/548M [00:54<00:00, 10.1MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 61.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a663bd-7010-44aa-a9a2-6f618e9b0aaf",
   "metadata": {},
   "source": [
    "## Greedy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0316f50f-9f34-4750-b584-7c3cf37f0f44",
   "metadata": {},
   "source": [
    "Greedy search is the simplest decoding method. It selects the word with the highest probability as its next word: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fe7ae-27d1-498f-ac17-9cea1f524ddc",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](images/greedy_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14a422-4370-4ae0-a463-e268eea5d10e",
   "metadata": {},
   "source": [
    "Starting from the word \"The\", the algorithm greedily chooses the next word of highest probability \"nice\" and so on, so that the final generated word sequence is (\"The\", \"nice\", \"woman\")\n",
    "having an overall probability of 0.5×0.4=0.2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65f5711-467a-495c-a9c8-1036f6295060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)\n",
    "\n",
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064e2ef-7938-4a9d-9783-10eff8d24514",
   "metadata": {},
   "source": [
    "We have generated our first short text with GPT2!\n",
    "\n",
    "The generated words following the context are reasonable, but the model quickly start repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search. The major drawback of greedy search though is that it misses high probability words hidden behind low probability word as can be seen in the sketch above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e65161-483d-42ea-a8ba-8d3129da43d6",
   "metadata": {},
   "source": [
    "The word \"has\" with its high conditional probability of 0.9 hidden behind the word \"dog\", which has only the second-highest conditional probability, so that greedy search misses the word sequence \"The\", \"dog\", \"has\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a3e34-6f24-4cf7-9775-dc2ea1a649b3",
   "metadata": {},
   "source": [
    "## **Beam Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79315d3d-dc95-4a5e-9563-50842ebaf327",
   "metadata": {},
   "source": [
    "Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Let's illustrate with num_beams=2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e572d2d4-cee0-47d7-94b3-f5644f802a56",
   "metadata": {
    "tags": []
   },
   "source": [
    "![title](images/beam_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffc88e-f797-4880-957a-9c05a3d3ba1b",
   "metadata": {},
   "source": [
    "At time step 1, besides the most likely hypothesis (\"The\", \"nice\"), beam search also keeps track of the second most likely one (\"The\", \"dog\"). At time step 2, beam search finds that the word sequence (\"The\", \"dog\", \"has\") with 0.36 higher probability than (\"the\", \"nice\", \"woman\") which has 0.2. Great, it has found the most likely word sequence in the example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a3455f-1b88-43cf-a93c-9967ee70cc58",
   "metadata": {},
   "source": [
    "Beam search will always find an output sequence with higher probability than greedy search, but its not guaranteed to find the most likely output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58ddaf74-90df-4136-9f09-0e6cae82adfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I don't think I'll ever be able to walk with my dog again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with my dog again, but I don\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f62ba1d-8291-4765-b58b-e4749ca4797a",
   "metadata": {},
   "source": [
    "While the result is arguably more fluent, the output still includes repetitions of the same word sequences. One of the available remedies is to introduce n-grams. The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db2fa07e-0ae8-4e26-bbb9-7a0d4f9e2954",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965e36d-4e22-422e-a8c3-a6633cd82737",
   "metadata": {},
   "source": [
    "We can see that the repetition does not appear anymore. Nevertheless, n-gram penalties have to be used with care. An article generated about the city New York should not use a 2-gram penalty or otherwise, the name of the city would only appear once in the whole text!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d70f7-2fd6-4587-aa6f-90473ee4790d",
   "metadata": {},
   "source": [
    "Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ff126-7997-4cac-955f-549e0fb1778c",
   "metadata": {},
   "source": [
    "In transformers, we simply set the parameter num_return_sequences to the number of highest scoring beams that should be returned. Make sure though that num_return_sequences <= num_beams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5896d3b-4727-4b71-bf30-43357ae9a030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea to\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time to take a\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea.\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96805a7-b92f-4c9b-ad54-36421dc4f26b",
   "metadata": {},
   "source": [
    "As can be seen, the five beam hypotheses are only marginally different to each other - which should not be too surprising when using only 5 beams."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8106a01c-4b01-404a-8228-584d8ace584a",
   "metadata": {},
   "source": [
    "## **Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b62940-2351-460d-9619-6e1b194dfd62",
   "metadata": {},
   "source": [
    "In its most basic form, sampling means randomly picking the next word according to its conditional probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e090ac-a46a-4e64-a4b4-aafd8bc5956b",
   "metadata": {},
   "source": [
    "![title](images/sampling_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49271681-5186-4f2d-8b5d-ff5039043f04",
   "metadata": {},
   "source": [
    "It becomes obvious that language generation using sampling is not deterministic anymore. The word (\"car\") is sampled from the conditioned probability distribution P(w|\"The\"), followed by sampling (\"drives\") from P(w|\"The\", \"car\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fabe40a-f6d5-418d-84e3-073ec72ccb4c",
   "metadata": {},
   "source": [
    "In transformers, we set do_sample=True and deactivate Top-K sampling (more on this later) via top_k=0. In the following, we will fix the random seed for illustration purposes. Feel free to change the set_seed argument to obtain different results, or to remove it for non-determinism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b684164a-89b5-4f73-8f72-5d2d860ce831",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93401f2-5655-4c1b-b181-1f5e3710c5b2",
   "metadata": {},
   "source": [
    "Interesting! The text seems alright - but when taking a closer look, it is not very coherent and doesn't sound like it was written by a human. That is the big problem when sampling word sequences: The models often generate incoherent gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe34e2-5f11-4a30-9ca2-a5358541ea2a",
   "metadata": {},
   "source": [
    "A trick is to make the distribution sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so called temperature of the softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9206ed67-89cf-4bd9-9edd-d2d2f22d20d0",
   "metadata": {},
   "source": [
    "<img src=\"images/sampling_search_with_temp.png\" width=\"800\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c911b9-5194-4151-8bcc-2d848e4ec0b7",
   "metadata": {},
   "source": [
    "The conditional next word distribution of step T=1 becomes much sharper leaving almost no chance for word (\"car\") to be selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cabd91e2-a964-4b54-804a-a4a7c0a0f18f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but I also love the fact that my cat is not a dog. She is a good, loving dog. I do not like to be held back by other dogs but I think that I have to\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae3dd0-f247-4477-8ca7-12c2fbfa4dba",
   "metadata": {},
   "source": [
    "There were less weird n-grams and the output is a bit more coherent now. However, while applying temperature can make a distribution less random, in its limit, when setting temperature -> 0, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09cffe-d011-49c6-b738-1da4fa386590",
   "metadata": {},
   "source": [
    "## **Top-K Sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730ca8f-ce6c-44d5-9937-02dd2223d10d",
   "metadata": {},
   "source": [
    "In Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34909ba7-fb22-4d09-aa28-d0e34fdf416d",
   "metadata": {},
   "source": [
    "<img src=\"images/top_k_sampling.png\" width=\"1200\" height=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d297e00-67cf-40d7-8342-661202a5e70b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog is I see a beautiful pet being cared for – I love having the opportunity to see her every day so I feel very privileged to have\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=35,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194e9be-ff74-4918-a098-47e5c7291bd0",
   "metadata": {},
   "source": [
    "Not bad at all! The text is arguably the most human-sounding text so far. One concern though with Top-K sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution. This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a411c9e-66f8-4456-b274-1ee771988ebe",
   "metadata": {},
   "source": [
    "## **Top-p (nucleus) sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad2632-1170-4a07-948b-cebd011a6b8c",
   "metadata": {},
   "source": [
    "Instead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Ok, that was very wordy, let's visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d5d9bc-5395-493a-9bfd-78ca0616ca97",
   "metadata": {},
   "source": [
    "<img src=\"images/top_p_sampling.png\" width=\"1200\" height=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dc378-e883-446e-a8cb-46f4fa7bc133",
   "metadata": {},
   "source": [
    "Having set p = 0.92, Top-p sampling picks the minimum number of words to exceed together p = 92% of the probability mass. In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb82c10b-2abe-4321-a785-632a4079dae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=35,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0d1236-3e7b-41b1-995f-3137f644cb67",
   "metadata": {},
   "source": [
    "While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af5fc26-1a29-4128-9aeb-4dbae20cb4fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog but sometimes I get nervous when she is around. I've been told that with her alone, she will usually wander off and then try to chase me. It's nice to know that I have this\n",
      "1: I enjoy walking with my cute dog. I think she is the same one I like to walk with my dog, I think she is about as girly as my first dog. I hope we can find an apartment for her when we\n",
      "2: I enjoy walking with my cute dog, but there's so much to say about him that I am going to miss it all. He has been so supportive and even had my number in his bag.\n",
      "\n",
      "I hope I can say\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(42)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5498a9-8fa9-4eef-987f-e58b824f0879",
   "metadata": {},
   "source": [
    "# **Dialogue Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3270f6-d29b-49f9-a7e5-72519a7faa43",
   "metadata": {},
   "source": [
    "In this use case, we will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e802765-6ded-4432-b620-11f9baad4649",
   "metadata": {},
   "source": [
    "Let's upload some simple dialogues from the DialogSum Hugging Face dataset. This dataset contains 10.000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd9b5056-8e47-40cb-ad13-b96b9783c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38ecc2b7-676c-412f-aeb5-912767751f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.58k/4.58k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 11.3M/11.3M [00:02<00:00, 4.12MB/s]\n",
      "Downloading data: 100%|██████████| 442k/442k [00:00<00:00, 458kB/s]t]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:01<00:00, 1.01MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:05<00:00,  1.70s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 85.79it/s]\n",
      "Generating train split: 12460 examples [00:00, 26184.78 examples/s]\n",
      "Generating validation split: 500 examples [00:00, 10931.04 examples/s]\n",
      "Generating test split: 1500 examples [00:00, 24702.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "huggingface_dataset_name = 'knkarthick/dialogsum'\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cafeed4a-0ad3-4180-be88-6c6b97085973",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = [40, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a411d4a8-0bc1-42d9-80c2-8e9b1b4d9a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dash_line = \"-\".join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print('Example ', i+1)\n",
    "    print(dash_line)\n",
    "    print('Input Dialogue: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print('Baseline Human Summary: ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash_line)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8ea871-a66d-4ed2-b1e7-ed703930dfd6",
   "metadata": {},
   "source": [
    "## **FLAN-T5 Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23f87-1cf0-4e65-bfc4-3606c32fbac6",
   "metadata": {},
   "source": [
    "<img src=\"images/flan2_architecture.jpg\" width=\"1000\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a51b3ea-8b5f-4749-acb4-b36083bebe57",
   "metadata": {},
   "source": [
    "#### Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d95e03b-8148-418c-9259-114693afd5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.40k/1.40k [00:00<00:00, 359kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 990M/990M [02:38<00:00, 6.26MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<00:00, 29.2kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.54k/2.54k [00:00<00:00, 508kB/s]\n",
      "Downloading spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 3.48MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.42M/2.42M [00:01<00:00, 1.78MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.20k/2.20k [00:00<00:00, 367kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9248fbf-1a34-4f29-90d0-070b1b126f2d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e4ccb4-e1ab-47be-8578-330a8428da07",
   "metadata": {},
   "source": [
    "<img src=\"images/flan_t5_tasks.png\" width=\"900\" height=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd6c0c-a017-41ec-b760-7d9cfc2b55cf",
   "metadata": {},
   "source": [
    "#### These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35639b0a-188c-4333-917f-2843259a7cea",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24014c68-c37c-4acb-81f3-b08ffb476ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"What time is it, Tom?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c0ca1ec-8652-4eeb-9fc6-3edc65dd806c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      " tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "DECODED SENTENCE: What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "print(f'ENCODED SENTENCE:\\n {sentence_encoded[\"input_ids\"][0]}')\n",
    "print(f'DECODED SENTENCE: {sentence_decoded}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc69b96d-4fb1-4ebd-9834-e0385dca2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dialogues(example_indices, dataset, prompt = \"%s\"):\n",
    "    for i, index in enumerate(example_indices):\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "    \n",
    "        input = prompt % (dialogue)\n",
    "        \n",
    "        inputs = tokenizer(input, return_tensors='pt')\n",
    "        pred = model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0]\n",
    "        output = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "    \n",
    "        print(dash_line)\n",
    "        print(f'Example {i+1}')\n",
    "        print(dash_line)\n",
    "        print(f'Input Prompt: \\n {dialogue}')\n",
    "        print(dash_line)\n",
    "        print(f'Baseline Human Summary: \\n {summary}')\n",
    "        print(dash_line)\n",
    "        print(f'Model Generation: \\n{output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa3b893f-c33f-4084-a61a-eafd80bec0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "Person1: It's ten to nine.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013b2c41-41c5-4d5c-b1db-11eb3eefb906",
   "metadata": {},
   "source": [
    "### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0849bb4-948f-48dd-af41-0d0e52252715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation. \n",
      "%s\n",
      "Summary:\n"
     ]
    }
   ],
   "source": [
    "prompt = f'Summarize the following conversation. \\n%s\\nSummary:'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b179486b-f899-4fb7-b04b-c514c5d1e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "The train is about to leave.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "#Person1#: I'm thinking of upgrading my computer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb3417f3-9ca3-425a-820d-ae1bf425a1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: \n",
      "%s\n",
      "\n",
      "What Happened?\n"
     ]
    }
   ],
   "source": [
    "prompt = f'Dialogue: \\n%s\\n\\nWhat Happened?'\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e845923b-e26d-4c1e-9d69-a7a9e4e70450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "Tom is late.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt: \n",
      " #Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation: \n",
      "#Person1#: You'd probably need a faster processor, more memory and a faster modem. #Person2#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e38693-945c-4000-bd83-5912db40047a",
   "metadata": {},
   "source": [
    "# One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f58f375e-557d-4a14-aefa-ad97f264f622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        prompt += f\"\"\"Dialogue:\\n{dialogue}\\n\\nWhat was going on?\\n{summary}\\n\\n\\n\"\"\"\n",
    "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "    prompt += f'Dialogue:\\n{dialogue}\\n\\nWhat was going on?'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c70dcf8-1a76-4dbb-ae4f-d94212f73660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfff3608-270f-4415-ab80-84118ff1845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - One Shot:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'Model Generation - One Shot:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e758b8a-5e70-4562-987c-6fba74c087c2",
   "metadata": {},
   "source": [
    "# Few Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3790c048-358f-4e04-9306-57d24d759c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a22b0ef-d18a-4915-8b51-1f909bdb6ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (818 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - Few Shot:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash_line)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'Model Generation - Few Shot:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c6aed0-085f-4d58-a8e9-1d1b00ae9b91",
   "metadata": {},
   "source": [
    "# Model Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4abde7b9-a0f0-4f74-a0aa-eaf4994d428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75512899-12e6-43fd-98f4-9c7d09847510",
   "metadata": {},
   "source": [
    "## Load Dataset and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa593aee-5a96-46de-8160-bb16abe96401",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_dataset_name = \"knkarthick/dialogsum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3a51cc8-d8d3-48b5-a9d0-1da6ad3ae31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(hugging_face_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bc43cb5-7ae6-4921-936a-f6962052d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/flan-t5-base'\n",
    "\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(torch_device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7ade33c4-b302-4c31-9ff6-c86bfa0efac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_trainable_model_parameters(model):\n",
    "        trainable_model_params = 0\n",
    "        all_model_params = 0\n",
    "        for _, param in model.named_parameters():\n",
    "            all_model_params += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_model_params += param.numel()\n",
    "        result = f\"trainable model parameters: {trainable_model_params}\\n\"\n",
    "        result += f\"all model parameters: {all_model_params}\\n\"\n",
    "        result += f\"Percentage of model params: {(trainable_model_params/all_model_params)*100}\"\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92f1c6a0-f76b-462d-860d-bda4b93a0836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "Percentage of model params: 100.0\n"
     ]
    }
   ],
   "source": [
    "print(number_of_trainable_model_parameters(original_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad2c79-e6ce-49ad-856a-588f61635429",
   "metadata": {},
   "source": [
    "## Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7fefc6a-53a9-4500-b598-a9ca00a3c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "\n",
      "Summarize the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - Zero Shot: \n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    original_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Model Generation - Zero Shot: \\n{output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22088e95-6104-486c-be81-81194e1fded1",
   "metadata": {},
   "source": [
    "## Perform Full Fine-Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57bd6c4-4932-49b4-b4a1-32a10ce8b533",
   "metadata": {},
   "source": [
    "### Preprocess the Dialog-Summary dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8012d-3343-4df0-9063-1aebcdd12bc4",
   "metadata": {},
   "source": [
    "Convert the dialog-summary (prompt-response) pairs into explicit instructions for the LLM. Prepend an instruction to the start of the dialog with 'Summarize the following conversation' and the start of the summary with 'Summary as follows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd762da7-873a-452a-ae18-de765b32a23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = 'Summarize the following conversation.\\n\\n'\n",
    "    end_prompt = '\\n\\nSummary: '\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    example['labels'] = tokenizer(example[\"summary\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f007400f-2603-4ed8-bf20-8ec54199576e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12460/12460 [00:07<00:00, 1675.51 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1713.19 examples/s]\n",
      "Map: 100%|██████████| 1500/1500 [00:00<00:00, 1795.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# The dataset actually contains 3 diff splits: train, validation, test\n",
    "# The tokenize_function code is handling all data accross all splits in batches\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeb5aa-bf87-45b0-b90d-a01fcf744d7c",
   "metadata": {},
   "source": [
    "To save some time, we will subsample the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e36a547-96bf-4bfc-8f4b-bbeed5096e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:07<00:00, 1566.54 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 1348.14 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:00<00:00, 1512.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ad14994-44d2-4be0-bbd1-4930cf8b0868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (125, 2)\n",
      "Validation: (5, 2)\n",
      "Test: (15, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 125\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e16f70-8638-4e8b-971c-16a09ad9dd13",
   "metadata": {},
   "source": [
    "### Fine-Tune the model with the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7297af38-81f9-4a3e-8394-32e2f536b1af",
   "metadata": {},
   "source": [
    "Now utilize the built-in Hugging Face Trainer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "210e9712-d0a5-43a4-b041-0284f44f057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=original_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f8bcb-eea6-48ce-90bc-490ea7175f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc796ce0-a496-428c-85eb-f3b7f18a4f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained('full/').to(torch_device)\n",
    "original_model = original_model.to(torch_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c322fc2-062c-443d-a8b1-eddd3ca7741a",
   "metadata": {},
   "source": [
    "## Evaluate the Model Qualitatively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51766726-3189-4b08-82cc-ab9ccab2ede1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "\n",
      "Summarize the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original Model Generation - Zero Shot: \n",
      "#Person1#: You can consider upgrading your system to a more powerful and more powerful hard disk.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Generation - Fine Tune: \n",
      "#Person1# suggests #Person2# adding a painting program to #Person2#'s software and upgrading the hardware. #Person1# also suggests #Person2# add a CD-ROM drive to #Person2#'s computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model Generation - Fine Tune: \\n{instruct_text_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ce6bb6-0889-4fe2-9771-a5fc50560215",
   "metadata": {},
   "source": [
    "## Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be96e9ae-014c-4e89-b80e-1c97aa78ae8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.27k/6.27k [00:00<00:00, 6.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db6a8655-c9cd-4c12-ae09-81ad9532bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "    original_model_summaries.append(original_text_output)\n",
    "\n",
    "    instruct_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    instruct_text_output = tokenizer.decode(instruct_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_summaries.append(instruct_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'instruct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "821b77dd-2618-4d82-bc00-f6c5fda06ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>human</th>\n",
       "      <th>original</th>\n",
       "      <th>instruct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>Employees are required to use instant messagin...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>This memo will be sent to all employees by thi...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>Employees are required to use the Office of In...</td>\n",
       "      <td>#Person1# asks Ms. Dawson to take a dictation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>People are talking about the traffic in this c...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>#Person1: I'm finally here!</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person1: I'm sorry to hear that you're stuck ...</td>\n",
       "      <td>#Person2# got stuck in traffic again. #Person1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are divorced.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>#Person1: Masha and Hero are getting a divorce.</td>\n",
       "      <td>#Person1# tells Kate Masha and Hero are gettin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Brian, thank you for coming to the ...</td>\n",
       "      <td>Brian's birthday is coming. Brian dances with ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               human  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                                            original  \\\n",
       "0  Employees are required to use instant messagin...   \n",
       "1  This memo will be sent to all employees by thi...   \n",
       "2  Employees are required to use the Office of In...   \n",
       "3  People are talking about the traffic in this c...   \n",
       "4                        #Person1: I'm finally here!   \n",
       "5  #Person1: I'm sorry to hear that you're stuck ...   \n",
       "6                       Masha and Hero are divorced.   \n",
       "7                       Masha and Hero are divorced.   \n",
       "8    #Person1: Masha and Hero are getting a divorce.   \n",
       "9  #Person1#: Brian, thank you for coming to the ...   \n",
       "\n",
       "                                            instruct  \n",
       "0  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "1  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "2  #Person1# asks Ms. Dawson to take a dictation ...  \n",
       "3  #Person2# got stuck in traffic again. #Person1...  \n",
       "4  #Person2# got stuck in traffic again. #Person1...  \n",
       "5  #Person2# got stuck in traffic again. #Person1...  \n",
       "6  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "7  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "8  #Person1# tells Kate Masha and Hero are gettin...  \n",
       "9  Brian's birthday is coming. Brian dances with ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b55838ad-c8bc-4e05-a94a-ddff480da233",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d075dc6-4c89-4ea4-b8bb-0b81e5fe9cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: \n",
      "{'rouge1': 0.261052062988671, 'rouge2': 0.08531489481944488, 'rougeL': 0.224821552384684, 'rougeLsum': 0.22788611265447228}\n",
      "Instruct Model: \n",
      "{'rouge1': 0.38857220563277894, 'rouge2': 0.13135692283806472, 'rougeL': 0.28167162470172985, 'rougeLsum': 0.28344342480768214}\n"
     ]
    }
   ],
   "source": [
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(f\"Original Model: \\n{original_model_results}\")\n",
    "print(f\"Instruct Model: \\n{instruct_model_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13167f8b-86c3-4c2f-b7b6-b615b78178c7",
   "metadata": {},
   "source": [
    "# Parameter Efficient Fine Tunning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f46afd4-77d4-42bd-ae16-58cfc7932ca4",
   "metadata": {},
   "source": [
    "Now lets perform Parameter Efficient Fine-Tunning (PEFT). Opposed to full fine tunning, PEFT is a form of instruction fine-tunnin hat is much more efficient than full fine-tunning - with comparable evaluation results as you will see soon.\n",
    "\n",
    "PEFT is a generic term that includes Low-Rank Adaptation (LoRA) and prompt tunning (which is not the same as prompt engineering). In most cases when someone says PEFT, they typically mean LoRA, at a very high level allows the user to fine-tune their model using fewer compute resources (in some cases, a single GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8166e-6506-4ad8-a1b0-b1256965a65e",
   "metadata": {},
   "source": [
    "## Setup the PEFT/LoRA model for Fine-Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5747ce62-a989-4221-8533-db2d8af27f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ba24b4e-5bba-465e-90c0-054bea5a24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q','v'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1122785-3957-41a9-b1d9-844d7d7da279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "Percentage of model params: 1.4092820552029972\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "print(number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f35033e6-9d34-4874-8f30-da488b5001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./peft-dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88da1282-c082-488b-a831-cdffc948e1cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "601c74cb-571b-46be-affc-13c4f995309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(\n",
    "    peft_model_base,\n",
    "    \"peft/\"\n",
    ").to(torch_device)\n",
    "original_model = original_model.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d51e485a-a14b-4150-af1d-681509b8a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Input Prompt:\n",
      "\n",
      "Summarize the following conversation\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Original Model Generation - Zero Shot: \n",
      "#Person2#: I'm not sure what exactly I need. #Person1: I'm not sure what I need. #Person2: I'm just not sure. #Person1#: I'm not sure what I need. #Person1: I'm not sure what I need. #Person2: I'm not sure what I need. #Person1: I'm just not sure what I need. #Person2: I'm just not sure what I need. #Person1: I'm just not sure what I need. #Person2: I'm just not sure what I need. #Person2: I'm just not sure what I need. #Person1: I'm just not sure what I need.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Instruct Model Generation - Zero Shot: \n",
      "#Person1# suggests #Person2# upgrading #Person2#'s system and #Person2# thinks it's a definite bonus. #Person1# also suggests adding a painting program to #Person2#'s software. #Person2# also wants to upgrade #Person2#'s hardware.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "human_baseline_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "\n",
    "original_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "original_text_output = tokenizer.decode(original_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join(\"\" for x in range(100))\n",
    "print(dash_line)\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print(dash_line)\n",
    "print(f\"Baseline Human Summary:\\n{human_baseline_summary}\\n\")\n",
    "print(dash_line)\n",
    "print(f\"Original Model Generation - Zero Shot: \\n{original_text_output}\")\n",
    "print(dash_line)\n",
    "print(f\"Instruct Model Generation - Zero Shot: \\n{peft_text_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aeaeac7f-aa42-4e44-afd8-83ae481c8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = dataset['test'][0:10]['dialogue']\n",
    "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "for _, dialogue in enumerate(dialogue):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "    \n",
    "    peft_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_text_output = tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_text_output)\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    "\n",
    "df = pd.DataFrame(zipped_summaries, columns=['human', 'original', 'peft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8def102-fa83-4131-868d-6c8313adc587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model: \n",
      "{'rouge1': 0.261052062988671, 'rouge2': 0.08531489481944488, 'rougeL': 0.224821552384684, 'rougeLsum': 0.22788611265447228}\n",
      "Instruct Model: \n",
      "{'rouge1': 0.38857220563277894, 'rouge2': 0.13135692283806472, 'rougeL': 0.28167162470172985, 'rougeLsum': 0.28344342480768214}\n",
      "Peft Model: \n",
      "{'rouge1': 0.33176278482581173, 'rouge2': 0.08811333505050914, 'rougeL': 0.2509677309788697, 'rougeLsum': 0.25262149176905513}\n"
     ]
    }
   ],
   "source": [
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "print(f\"Original Model: \\n{original_model_results}\")\n",
    "print(f\"Instruct Model: \\n{instruct_model_results}\")\n",
    "print(f\"Peft Model: \\n{peft_model_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a2b220-62fa-4473-9d5c-fb741929b5a7",
   "metadata": {},
   "source": [
    "# **Knowledge Grounding**\n",
    "\n",
    "Knowledge grounding in Natural Language Processing (NLP) refers to the process of connecting or linking information in text to real-world entities or concepts. It involves ensuring that the language model understands and can relate the information it processes to factual, contextual, or external knowledge.\n",
    "\n",
    "For instance, if a sentence mentions \"Einstein's theory of relativity,\" knowledge grounding would involve the model recognizing that Einstein refers to a renowned physicist and the theory of relativity is a fundamental concept in physics.\n",
    "\n",
    "Knowledge grounding is crucial for NLP applications that require a deeper understanding of the world, as it enables the model to go beyond surface-level patterns in text and make meaningful inferences based on its understanding of the underlying concepts. This is particularly important in tasks like question-answering, where the model needs to provide accurate and contextually relevant responses.\n",
    "Knowledge grounding with Retrieval Augmented Generation (RAG) is implemented to mitigate hallucinations and provide trustworthy and reliable responses. This is achieved by incorporating information from external sources to validate and support the generated text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6c6de6-1c48-476e-9ad7-68bc5853dadf",
   "metadata": {},
   "source": [
    "\n",
    "## To know more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a2873-ff40-45e3-874c-bf6e42c264a9",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52624d5e-6c78-4a20-ba5d-92213c7f22cc",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/build-industry-specific-llms-using-retrieval-augmented-generation-af9e98bb6f68"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7bcfd7-faac-4495-9bd5-6496e1c772a4",
   "metadata": {},
   "source": [
    "### Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52231ff6-67e1-4489-9b13-d6d4e7f22a55",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/get_started/introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76de6728-0bb3-4112-a66f-159e9d7843ce",
   "metadata": {},
   "source": [
    "### OpenAI API Key\n",
    "\n",
    "Note: Please note that the use of the OpenAI API may require the utilization of allocated free credits or additional purchases for implementing the project. Kindly take note of the free credits limit provided for your usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0e4ea-4f39-483c-b523-fdc2184137b6",
   "metadata": {},
   "source": [
    "https://www.howtogeek.com/885918/how-to-get-an-openai-api-key/#:~:text=Go%20to%20OpenAI's%20Platform%20website,generate%20a%20new%20API%20key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fac159-3d3f-43d8-8bb5-947ff2bcffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.27.12.174:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "a5255313ec7d762337e1d1f02b6be7c9\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Printable Design for Apparel Products\n",
      "\n",
      "Available in: AU, CA, EU, NZ, US with minimum quantity per order of 1 piece and maximum of 50 pieces. Printing goes direct to garment printing. Adult T Shirts for Men and Women are available in sizes S, M, L, XL, XXL in white, black, navy, light steel, deep royal, deep red, smoke gray and ash colors. Kids T Shirts are available in Youth (S, M and L) in colors white, black, navy, gray. Royal Blue, Red and Charcoal Kids T Shirts are only available in CA and US. Toddlers T shirts are available in sizes 2, 3 and 4 and white, black, navy, gray, light blue, pink colors. Charcoal Toddler T-Shirt is only available in CA and US. Infant onesies are available in 4 different sizes (New Born, 6 Months, 12 months and 18 Months) in White, Black, NAvy, Gray, Light Blue, Pink colors. Sweatshirts are available in sizes S, M, L, XL, XXL and Black, Navy, Heather Charcoal (only AU, CA, NZ, US), Heather Gray (AU and NZ only) and Gray (EU Only)\n",
      "\n",
      "Title: Printable Design for Paper Products\n",
      "\n",
      "Overview\n",
      "\n",
      "At the moment, the design types in the table below are available in Our Shop Print. However, not all regions support all these design types. As Print continues to expand, eventually, they will be available to all. Please see the list of countries per served region.\n",
      "\n",
      "Print service in Russia and South Korea is temporarily available until further notice.\n",
      "Human: What are the adult T-Shirt Sizes in US?\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Printable Design for Apparel Products\n",
      "\n",
      "Available in: AU, CA, EU, NZ, US with minimum quantity per order of 1 piece and maximum of 50 pieces. Printing goes direct to garment printing. Adult T Shirts for Men and Women are available in sizes S, M, L, XL, XXL in white, black, navy, light steel, deep royal, deep red, smoke gray and ash colors. Kids T Shirts are available in Youth (S, M and L) in colors white, black, navy, gray. Royal Blue, Red and Charcoal Kids T Shirts are only available in CA and US. Toddlers T shirts are available in sizes 2, 3 and 4 and white, black, navy, gray, light blue, pink colors. Charcoal Toddler T-Shirt is only available in CA and US. Infant onesies are available in 4 different sizes (New Born, 6 Months, 12 months and 18 Months) in White, Black, NAvy, Gray, Light Blue, Pink colors. Sweatshirts are available in sizes S, M, L, XL, XXL and Black, Navy, Heather Charcoal (only AU, CA, NZ, US), Heather Gray (AU and NZ only) and Gray (EU Only)\n",
      "\n",
      "Title: Printable Design for Paper Products\n",
      "\n",
      "Overview\n",
      "\n",
      "At the moment, the design types in the table below are available in Our Shop Print. However, not all regions support all these design types. As Print continues to expand, eventually, they will be available to all. Please see the list of countries per served region.\n",
      "\n",
      "Print service in Russia and South Korea is temporarily available until further notice.\n",
      "Human: What are toddlers t-shirt sizes available by country?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the users question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "Title: Printable Design for Paper Products\n",
      "\n",
      "Overview\n",
      "\n",
      "At the moment, the design types in the table below are available in Our Shop Print. However, not all regions support all these design types. As Print continues to expand, eventually, they will be available to all. Please see the list of countries per served region.\n",
      "\n",
      "Print service in Russia and South Korea is temporarily available until further notice.\n",
      "\n",
      "Printable Design for Apparel Products\n",
      "\n",
      "Available in: AU, CA, EU, NZ, US with minimum quantity per order of 1 piece and maximum of 50 pieces. Printing goes direct to garment printing. Adult T Shirts for Men and Women are available in sizes S, M, L, XL, XXL in white, black, navy, light steel, deep royal, deep red, smoke gray and ash colors. Kids T Shirts are available in Youth (S, M and L) in colors white, black, navy, gray. Royal Blue, Red and Charcoal Kids T Shirts are only available in CA and US. Toddlers T shirts are available in sizes 2, 3 and 4 and white, black, navy, gray, light blue, pink colors. Charcoal Toddler T-Shirt is only available in CA and US. Infant onesies are available in 4 different sizes (New Born, 6 Months, 12 months and 18 Months) in White, Black, NAvy, Gray, Light Blue, Pink colors. Sweatshirts are available in sizes S, M, L, XL, XXL and Black, Navy, Heather Charcoal (only AU, CA, NZ, US), Heather Gray (AU and NZ only) and Gray (EU Only)\n",
      "Human: What are the envelope types available?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!streamlit run llm_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5042773",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In today's world, we see language models doing some pretty amazing things. They help businesses understand text on a big scale and make our online experiences better.\n",
    "\n",
    "This project was all about getting to know these language models inside out. We looked at how they work, how we can use them better, and even gave them a bit of a tune-up. We made them smarter by teaching them how to answer questions and summarize conversations.\n",
    "\n",
    "We also built a cool shopping chatbot. This bot is smart because it doesn't make up stuff; it gives you real info about products.\n",
    "\n",
    "With this project, you've learned a lot about these language models, and you're all set to use them for exciting tasks, from writing text to building smart applications.\n",
    "\n",
    "\n",
    "Start experimenting !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8f361",
   "metadata": {},
   "source": [
    "## **Interview Questions**\n",
    "\n",
    "\n",
    "* Can you explain the significance of Large Language Models in today's data-driven world?\n",
    "* What is the main purpose of prompt engineering in the context of LLMs?\n",
    "* How does fine-tuning enhance the performance of a pre-trained language model?\n",
    "* What is the difference between full fine-tuning and Parameter Efficient Fine Tuning (PEFT)?\n",
    "* Can you explain the concept of Retrieval Augmented Generation (RAG) and how it was implemented in the project?\n",
    "* How does knowledge grounding improve the reliability of responses generated by the chatbot?\n",
    "* What is Reinforcement Learning from Human Feedback (RLHF)?\n",
    "\n",
    "* How did you evaluate the performance of the models in this project, and what metrics did you use?\n",
    "How did you handle the potential issue of hallucinations in the chatbot's responses?\n",
    "* What challenges did you encounter during the implementation of fine-tuning techniques, and how did you overcome them?\n",
    "* How would you address potential ethical concerns related to the use of language models in real-world applications?\n",
    "* What are some potential future improvements or extensions you would consider for this project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b809f1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
